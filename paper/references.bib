@misc{liu-2020-mbart,
  title         = {Multilingual Denoising Pre-training for Neural Machine Translation},
  author        = {Yinhan Liu and Jiatao Gu and Naman Goyal and Xian Li and Sergey Edunov and Marjan Ghazvininejad and Mike Lewis and Luke Zettlemoyer},
  year          = {2020},
  eprint        = {2001.08210},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2001.08210}
}

@misc{wikipedia-list-languages,
  author = {{Wikipedia contributors}},
  title  = {List of languages by number of native speakers --- {Wikipedia}{,} The Free Encyclopedia},
  year   = {2024},
  url    = {https://en.wikipedia.org/w/index.php?title=List_of_languages_by_number_of_native_speakers&oldid=1231985127},
  note   = {[Online; accessed 5-July-2024]}
}

@inproceedings{chan-etal-2020-german-bert,
  title     = {{G}erman{'}s Next Language Model},
  author    = {Chan, Branden  and
               Schweter, Stefan  and
               M{\"o}ller, Timo},
  editor    = {Scott, Donia  and
               Bel, Nuria  and
               Zong, Chengqing},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  month     = dec,
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2020.coling-main.598},
  doi       = {10.18653/v1/2020.coling-main.598},
  pages     = {6788--6796},
  abstract  = {In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.}
}