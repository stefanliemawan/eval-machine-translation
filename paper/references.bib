@misc{liu-2020-mbart,
  title         = {Multilingual Denoising Pre-training for Neural Machine Translation},
  author        = {Yinhan Liu and Jiatao Gu and Naman Goyal and Xian Li and Sergey Edunov and Marjan Ghazvininejad and Mike Lewis and Luke Zettlemoyer},
  year          = {2020},
  eprint        = {2001.08210},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2001.08210}
}

@misc{wikipedia-list-languages,
  author = {{Wikipedia contributors}},
  title  = {List of languages by number of native speakers --- {Wikipedia}{,} The Free Encyclopedia},
  year   = {2024},
  url    = {https://en.wikipedia.org/w/index.php?title=List_of_languages_by_number_of_native_speakers&oldid=1231985127},
  note   = {[Online; accessed 5-July-2024]}
}

@inproceedings{chan-etal-2020-german-bert,
  title     = {{G}erman{'}s Next Language Model},
  author    = {Chan, Branden  and
               Schweter, Stefan  and
               M{\"o}ller, Timo},
  editor    = {Scott, Donia  and
               Bel, Nuria  and
               Zong, Chengqing},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  month     = dec,
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2020.coling-main.598},
  doi       = {10.18653/v1/2020.coling-main.598},
  pages     = {6788--6796},
  abstract  = {In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.}
}

@book{ethnologue-2024,
  editor    = {Eberhard, David M. and Simons, Gary F. and Fennig, Charles D.},
  title     = {Ethnologue: Languages of the World},
  edition   = {27th},
  year      = {2024},
  publisher = {SIL International},
  address   = {Dallas, Texas},
  url       = {http://www.ethnologue.com}
}

@misc{raffel-2023-t5,
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year          = {2023},
  eprint        = {1910.10683},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1910.10683}
}

@article{weaver-1999,
  author  = {Weaver, Warren},
  title   = {Memorandum on Translation},
  journal = {MT News International},
  number  = {22},
  month   = {July},
  year    = {1999},
  pages   = {5--6, 15}
}

@misc{tatoeba,
  author = {{Tatoeba Community}},
  title  = {About Tatoeba},
  year   = 2024,
  url    = {https://tatoeba.org/en},
  note   = {Accessed: 2024-07-06}
}

@inproceedings{papieni-2002-bleu,
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  title     = {BLEU: a method for automatic evaluation of machine translation},
  year      = {2002},
  publisher = {Association for Computational Linguistics},
  address   = {USA},
  url       = {https://doi.org/10.3115/1073083.1073135},
  doi       = {10.3115/1073083.1073135},
  abstract  = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  pages     = {311–318},
  numpages  = {8},
  location  = {Philadelphia, Pennsylvania},
  series    = {ACL '02}
}

@inproceedings{lavie-2007-meteor,
  author    = {Lavie, Alon and Agarwal, Abhaya},
  title     = {Meteor: an automatic metric for MT evaluation with high levels of correlation with human judgments},
  year      = {2007},
  publisher = {Association for Computational Linguistics},
  address   = {USA},
  abstract  = {Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.},
  booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
  pages     = {228–231},
  numpages  = {4},
  location  = {Prague, Czech Republic},
  series    = {StatMT '07}
}

@article{stahlberg-2020-nmt-review,
  title     = {Neural Machine Translation: A Review},
  volume    = {69},
  issn      = {1076-9757},
  url       = {http://dx.doi.org/10.1613/jair.1.12007},
  doi       = {10.1613/jair.1.12007},
  journal   = {Journal of Artificial Intelligence Research},
  publisher = {AI Access Foundation},
  author    = {Stahlberg,  Felix},
  year      = {2020},
  month     = oct,
  pages     = {343–418}
}

@misc{wikipedia-google-translate,
  author = {{Wikipedia contributors}},
  title  = {Google Translate --- {Wikipedia}{,} The Free Encyclopedia},
  year   = {2024},
  url    = {https://en.wikipedia.org/w/index.php?title=Google_Translate&oldid=1232822378},
  note   = {[Online; accessed 6-July-2024]}
}