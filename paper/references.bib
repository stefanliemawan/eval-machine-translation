\@misc{liu-2020-mbart,
  archiveprefix = {arXiv},
  author        = {Yinhan Liu and Jiatao Gu and Naman Goyal and Xian Li and Sergey Edunov and Marjan Ghazvininejad and Mike Lewis and Luke Zettlemoyer},
  eprint        = {2001.08210},
  primaryclass  = {cs.CL},
  title         = {Multilingual Denoising Pre-training for Neural Machine Translation},
  url           = {https://arxiv.org/abs/2001.08210},
  year          = {2020}
}

@misc{wikipedia-list-languages,
  author = {{Wikipedia contributors}},
  note   = {[Online; accessed 5-July-2024]},
  title  = {List of languages by number of native speakers --- {Wikipedia}{,} The Free Encyclopedia},
  url    = {https://en.wikipedia.org/w/index.php?title=List_of_languages_by_number_of_native_speakers&oldid=1231985127},
  year   = {2024}
}

@inproceedings{chan-etal-2020-german-bert,
  abstract  = {In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.},
  address   = {Barcelona, Spain (Online)},
  author    = {Chan, Branden  and
               Schweter, Stefan  and
               M{\"o}ller, Timo},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  doi       = {10.18653/v1/2020.coling-main.598},
  editor    = {Scott, Donia  and
               Bel, Nuria  and
               Zong, Chengqing},
  month     = dec,
  pages     = {6788--6796},
  publisher = {International Committee on Computational Linguistics},
  title     = {{G}erman{'}s Next Language Model},
  url       = {https://aclanthology.org/2020.coling-main.598},
  year      = {2020}
}

@book{ethnologue-2024,
  address   = {Dallas, Texas},
  edition   = {27th},
  editor    = {Eberhard, David M. and Simons, Gary F. and Fennig, Charles D.},
  publisher = {SIL International},
  title     = {Ethnologue: Languages of the World},
  url       = {http://www.ethnologue.com},
  year      = {2024}
}

@misc{raffel-2023-t5,
  archiveprefix = {arXiv},
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  eprint        = {1910.10683},
  primaryclass  = {cs.LG},
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  url           = {https://arxiv.org/abs/1910.10683},
  year          = {2023}
}

@article{weaver-1999,
  author  = {Weaver, Warren},
  journal = {MT News International},
  month   = {July},
  number  = {22},
  pages   = {5--6, 15},
  title   = {Memorandum on Translation},
  year    = {1999}
}

@misc{tatoeba,
  author = {{Tatoeba Community}},
  note   = {Accessed: 2024-07-06},
  title  = {About Tatoeba},
  url    = {https://tatoeba.org/en},
  year   = 2024
}

@inproceedings{papieni-2002-bleu,
  abstract  = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  doi       = {10.3115/1073083.1073135},
  numpages  = {8},
  pages     = {311–318},
  publisher = {Association for Computational Linguistics},
  series    = {ACL '02},
  title     = {BLEU: a method for automatic evaluation of machine translation},
  url       = {https://doi.org/10.3115/1073083.1073135},
  year      = {2002}
}

@inproceedings{lavie-2007-meteor,
  abstract  = {Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.},
  author    = {Lavie, Alon and Agarwal, Abhaya},
  booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
  numpages  = {4},
  pages     = {228–231},
  publisher = {Association for Computational Linguistics},
  series    = {StatMT '07},
  title     = {Meteor: an automatic metric for MT evaluation with high levels of correlation with human judgments},
  year      = {2007}
}

@article{stahlberg-2020-nmt-review,
  author    = {Stahlberg,  Felix},
  doi       = {10.1613/jair.1.12007},
  issn      = {1076-9757},
  journal   = {Journal of Artificial Intelligence Research},
  month     = oct,
  pages     = {343–418},
  publisher = {AI Access Foundation},
  title     = {Neural Machine Translation: A Review},
  url       = {http://dx.doi.org/10.1613/jair.1.12007},
  volume    = {69},
  year      = {2020}
}

@misc{wikipedia-google-translate,
  author = {{Wikipedia contributors}},
  note   = {[Online; accessed 6-July-2024]},
  title  = {Google Translate --- {Wikipedia}{,} The Free Encyclopedia},
  url    = {https://en.wikipedia.org/w/index.php?title=Google_Translate&oldid=1232822378},
  year   = {2024}
}

@misc{deepl,
  author       = {{DeepL GmbH}},
  howpublished = {\url{https://www.deepl.com/en/translator}},
  note         = {Accessed: 2024-07-06},
  title        = {{DeepL Translator}}
}

@misc{fan-2020-m2m100,
  archiveprefix = {arXiv},
  author        = {Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},
  eprint        = {2010.11125},
  primaryclass  = {cs.CL},
  title         = {Beyond English-Centric Multilingual Machine Translation},
  url           = {https://arxiv.org/abs/2010.11125},
  year          = {2020}
}

@misc{nllb200-2020,
  archiveprefix = {arXiv},
  author        = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  eprint        = {2207.04672},
  primaryclass  = {cs.CL},
  title         = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  url           = {https://arxiv.org/abs/2207.04672},
  year          = {2022}
}

@misc{wei-2023-polylm,
  archiveprefix = {arXiv},
  author        = {Xiangpeng Wei and Haoran Wei and Huan Lin and Tianhao Li and Pei Zhang and Xingzhang Ren and Mei Li and Yu Wan and Zhiwei Cao and Binbin Xie and Tianxiang Hu and Shangjie Li and Binyuan Hui and Bowen Yu and Dayiheng Liu and Baosong Yang and Fei Huang and Jun Xie},
  eprint        = {2307.06018},
  primaryclass  = {cs.CL},
  title         = {PolyLM: An Open Source Polyglot Large Language Model},
  url           = {https://arxiv.org/abs/2307.06018},
  year          = {2023}
}

@misc{xue-2021-mt5,
  archiveprefix = {arXiv},
  author        = {Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
  eprint        = {2010.11934},
  primaryclass  = {cs.CL},
  title         = {mT5: A massively multilingual pre-trained text-to-text transformer},
  url           = {https://arxiv.org/abs/2010.11934},
  year          = {2021}
}

@misc{devlin-2019-bert,
  archiveprefix = {arXiv},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  eprint        = {1810.04805},
  primaryclass  = {cs.CL},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url           = {https://arxiv.org/abs/1810.04805},
  year          = {2019}
}

@inproceedings{wu-2020-mbert-are-all,
  abstract  = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
  address   = {Online},
  author    = {Wu, Shijie  and
               Dredze, Mark},
  booktitle = {Proceedings of the 5th Workshop on Representation Learning for NLP},
  doi       = {10.18653/v1/2020.repl4nlp-1.16},
  editor    = {Gella, Spandana  and
               Welbl, Johannes  and
               Rei, Marek  and
               Petroni, Fabio  and
               Lewis, Patrick  and
               Strubell, Emma  and
               Seo, Minjoon  and
               Hajishirzi, Hannaneh},
  month     = jul,
  pages     = {120--130},
  publisher = {Association for Computational Linguistics},
  title     = {Are All Languages Created Equal in Multilingual {BERT}?},
  url       = {https://aclanthology.org/2020.repl4nlp-1.16},
  year      = {2020}
}

@misc{openai-2024-gpt4,
  archiveprefix = {arXiv},
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  eprint        = {2303.08774},
  primaryclass  = {cs.CL},
  title         = {GPT-4 Technical Report},
  url           = {https://arxiv.org/abs/2303.08774},
  year          = {2024}
}

@inproceedings{vaswani-2017-attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@article{lopez-2008-smt,
  abstract   = {Statistical machine translation (SMT) treats the translation of natural language as a machine learning problem. By examining many samples of human-produced translation, SMT algorithms automatically learn how to translate. SMT has made tremendous strides in less than two decades, and new ideas are constantly introduced. This survey presents a tutorial overview of the state of the art. We describe the context of the current research and then move to a formal problem description and an overview of the main subproblems: translation modeling, parameter estimation, and decoding. Along the way, we present a taxonomy of some different approaches within these areas. We conclude with an overview of evaluation and a discussion of future directions.},
  address    = {New York, NY, USA},
  articleno  = {8},
  author     = {Lopez, Adam},
  doi        = {10.1145/1380584.1380586},
  issn       = {0360-0300},
  issue_date = {August 2008},
  journal    = {ACM Comput. Surv.},
  keywords   = {Natural language processing, machine translation},
  month      = aug,
  number     = {3},
  numpages   = {49},
  publisher  = {Association for Computing Machinery},
  title      = {Statistical machine translation},
  url        = {https://doi.org/10.1145/1380584.1380586},
  volume     = {40},
  year       = {2008}
}

@inproceedings{hutchins-2001-mt-50-years,
  author = {William J. Hutchins},
  title  = {Machine translation over fifty years},
  url    = {https://api.semanticscholar.org/CorpusID:6196527},
  year   = {2001}
}

@inproceedings{hutchins-2006-first-mt,
  author = {John Hutchins},
  title  = {The first public demonstration of machine translation : the Georgetown-IBM system, 7th January 1954},
  url    = {https://api.semanticscholar.org/CorpusID:132677},
  year   = {2006}
}

@article{wang-2022-progress,
  abstract = {After more than 70 years of evolution, great achievements have been made in machine translation. Especially in recent years, translation quality has been greatly improved with the emergence of neural machine translation (NMT). In this article, we first review the history of machine translation from rule-based machine translation to example-based machine translation and statistical machine translation. We then introduce NMT in more detail, including the basic framework and the current dominant framework, Transformer, as well as multilingual translation models to deal with the data sparseness problem. In addition, we introduce cutting-edge simultaneous translation methods that achieve a balance between translation quality and latency. We then describe various products and applications of machine translation. At the end of this article, we briefly discuss challenges and future research directions in this field.},
  author   = {Haifeng Wang and Hua Wu and Zhongjun He and Liang Huang and Kenneth Ward Church},
  doi      = {https://doi.org/10.1016/j.eng.2021.03.023},
  issn     = {2095-8099},
  journal  = {Engineering},
  keywords = {Machine translation, Neural machine translation, Simultaneous translation},
  pages    = {143-153},
  title    = {Progress in Machine Translation},
  url      = {https://www.sciencedirect.com/science/article/pii/S2095809921002745},
  volume   = {18},
  year     = {2022}
}

@inproceedings{hutchins-1998-development-mt,
  author = {John Hutchins},
  title  = {The development and use of machine translation systems and computer-based translation tools in Europe, Asia, and North America},
  url    = {https://api.semanticscholar.org/CorpusID:18918684},
  year   = {1998}
}

@inproceedings{hutchins-1994-research-methods-mt,
  author    = {John Hutchins},
  booktitle = {BCS International Academic Conference},
  title     = {Research methods and system designs in machine translation: a ten-year review, 1984-1994},
  url       = {https://api.semanticscholar.org/CorpusID:15952756},
  year      = {1994}
}

@misc{redokun-translation-statistics,
  author       = {Anastasia Popova},
  howpublished = {\url{https://redokun.com/blog/translation-statistics}},
  note         = {Accessed: 2024-07-11},
  title        = {Translation Statistics},
  year         = {2024}
}

@online{csa-2020,
  author = {{CSA Research}},
  note   = {Accessed: 2024-07-11},
  title  = {The Language Sector in Eight Charts},
  url    = {https://csa-research.com/Blogs-Events/Blog/ArticleID/785/The-Language-Sector-in-Eight-Charts},
  year   = {2020}
}

@report{intento-2020,
  institution = {Intento},
  note        = {Independent multi-domain evaluation of commercial Machine Translation engines},
  title       = {The State of Machine Translation 2020},
  url         = {https://try.inten.to/mt_report_2020},
  year        = {2020}
}

@misc{jiang-2023-mistral-7b,
  archiveprefix = {arXiv},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  eprint        = {2310.06825},
  primaryclass  = {cs.CL},
  title         = {Mistral 7B},
  url           = {https://arxiv.org/abs/2310.06825},
  year          = {2023}
}

@article{dabre-2020-multilingual-survey,
  address    = {New York, NY, USA},
  articleno  = {99},
  author     = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
  doi        = {10.1145/3406095},
  issn       = {0360-0300},
  issue_date = {September 2021},
  journal    = {ACM Comput. Surv.},
  keywords   = {Neural machine translation, low-resource, multi-source, multilingualism, survey, zero-shot},
  month      = {sep},
  number     = {5},
  numpages   = {38},
  publisher  = {Association for Computing Machinery},
  title      = {A Survey of Multilingual Neural Machine Translation},
  url        = {https://doi.org/10.1145/3406095},
  volume     = {53},
  year       = {2020}
}

@inproceedings{klein-etal-2017-opennmt,
  address   = {Vancouver, Canada},
  author    = {Klein, Guillaume  and
               Kim, Yoon  and
               Deng, Yuntian  and
               Senellart, Jean  and
               Rush, Alexander},
  booktitle = {Proceedings of {ACL} 2017, System Demonstrations},
  month     = jul,
  pages     = {67--72},
  publisher = {Association for Computational Linguistics},
  title     = {{O}pen{NMT}: Open-Source Toolkit for Neural Machine Translation},
  url       = {https://www.aclweb.org/anthology/P17-4012},
  year      = {2017}
}

@article{tiedemann-2023-democratizing,
  author    = {Tiedemann,  J\"{o}rg and Aulamo,  Mikko and Bakshandaeva,  Daria and Boggia,  Michele and Gr\"{o}nroos,  Stig-Arne and Nieminen,  Tommi and Raganato,  Alessandro and Scherrer,  Yves and Vázquez,  Raúl and Virpioja,  Sami},
  doi       = {10.1007/s10579-023-09704-w},
  issn      = {1574-0218},
  journal   = {Language Resources and Evaluation},
  month     = dec,
  number    = {2},
  pages     = {713–755},
  publisher = {Springer Science and Business Media LLC},
  title     = {Democratizing neural machine translation with OPUS-MT},
  url       = {http://dx.doi.org/10.1007/s10579-023-09704-w},
  volume    = {58},
  year      = {2023}
}

@inproceedings{tiedemann-2020-opus-mt,
  address   = {Lisbon, Portugal},
  author    = {J{\"o}rg Tiedemann and Santhosh Thottingal},
  booktitle = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation (EAMT)},
  title     = {{OPUS-MT} — {B}uilding open translation services for the {W}orld},
  year      = {2020}
}

@inproceedings{cho-2014-properties,
  address   = {Doha, Qatar},
  author    = {Cho, Kyunghyun  and
               van Merri{\"e}nboer, Bart  and
               Bahdanau, Dzmitry  and
               Bengio, Yoshua},
  booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
  doi       = {10.3115/v1/W14-4012},
  editor    = {Wu, Dekai  and
               Carpuat, Marine  and
               Carreras, Xavier  and
               Vecchi, Eva Maria},
  month     = oct,
  pages     = {103--111},
  publisher = {Association for Computational Linguistics},
  title     = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
  url       = {https://aclanthology.org/W14-4012},
  year      = {2014}
}

@misc{sutskever-2014-seq2seq,
  archiveprefix = {arXiv},
  author        = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  eprint        = {1409.3215},
  primaryclass  = {cs.CL},
  title         = {Sequence to Sequence Learning with Neural Networks},
  url           = {https://arxiv.org/abs/1409.3215},
  year          = {2014}
}

@inproceedings{okpor-2014-machine-ta,
  author = {Margaret Dumebi Okpor},
  title  = {Machine Translation Approaches: Issues and Challenges},
  url    = {https://api.semanticscholar.org/CorpusID:11483090},
  year   = {2014}
}

@misc{opus,
  author       = {OPUS},
  howpublished = {\url{https://opus.nlpl.eu/}},
  note         = {Accessed: 2024-07-29},
  title        = {OPUS: The Open Parallel Corpus},
  year         = {2024}
}

@article{ranathunga-2023-nmt-low-res,
  address    = {New York, NY, USA},
  articleno  = {229},
  author     = {Ranathunga, Surangika and Lee, En-Shiun Annie and Prifti Skenduli, Marjana and Shekhar, Ravi and Alam, Mehreen and Kaur, Rishemjit},
  doi        = {10.1145/3567592},
  issn       = {0360-0300},
  issue_date = {November 2023},
  journal    = {ACM Comput. Surv.},
  keywords   = {Neural machine translation, low-resource languages, unsupervised NMT, semi-supervised NMT, multilingual NMT, transfer learning, data augmentation, zero-shot translation, pivoting},
  month      = {feb},
  number     = {11},
  numpages   = {37},
  publisher  = {Association for Computing Machinery},
  title      = {Neural Machine Translation for Low-resource Languages: A Survey},
  url        = {https://doi.org/10.1145/3567592},
  volume     = {55},
  year       = {2023}
}

@misc{aharoni-2019-massively-multilingual,
  archiveprefix = {arXiv},
  author        = {Roee Aharoni and Melvin Johnson and Orhan Firat},
  eprint        = {1903.00089},
  primaryclass  = {cs.CL},
  title         = {Massively Multilingual Neural Machine Translation},
  url           = {https://arxiv.org/abs/1903.00089},
  year          = {2019}
}

@misc{koehn-2017-challenges,
  archiveprefix = {arXiv},
  author        = {Philipp Koehn and Rebecca Knowles},
  eprint        = {1706.03872},
  primaryclass  = {cs.CL},
  title         = {Six Challenges for Neural Machine Translation},
  url           = {https://arxiv.org/abs/1706.03872},
  year          = {2017}
}

@inproceedings{cooper-stickland-2021-recipes,
  author    = {Cooper Stickland, Asa and Li, Xian and Ghazvininejad, Marjan},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  doi       = {10.18653/v1/2021.eacl-main.301},
  publisher = {Association for Computational Linguistics},
  title     = {Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation},
  url       = {http://dx.doi.org/10.18653/v1/2021.eacl-main.301},
  year      = {2021}
}

@misc{garcia-2020-multilingual,
  archiveprefix = {arXiv},
  author        = {Xavier Garcia and Pierre Foret and Thibault Sellam and Ankur P. Parikh},
  eprint        = {2002.02955},
  primaryclass  = {cs.CL},
  title         = {A Multilingual View of Unsupervised Machine Translation},
  url           = {https://arxiv.org/abs/2002.02955},
  year          = {2020}
}

@article{goyal-2022-flores,
  author  = {Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc’Aurelio and Guzmán, Francisco and Fan, Angela},
  doi     = {10.1162/tacl_a_00474},
  eprint  = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00474/2020699/tacl\_a\_00474.pdf},
  issn    = {2307-387X},
  journal = {Transactions of the Association for Computational Linguistics},
  month   = {05},
  pages   = {522-538},
  title   = {{The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation}},
  url     = {https://doi.org/10.1162/tacl\_a\_00474},
  volume  = {10},
  year    = {2022}
}

@inproceedings{ye-2018-word-embeddings-ted,
  author    = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},
  booktitle = {HLT-NAACL},
  title     = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},
  year      = {2018}
}

@inproceedings{agarwal-iwstl-2023,
  author          = {Agarwal, Milind and Agrawal, Sweta and Anastasopoulos, Antonios and Bentivogli, Luisa and Bojar, Ondřej and Borg, Claudia and Carpuat, Marine and Cattoni, Roldano and Cettolo, Mauro and Chen, Mingda and Chen, William and Choukri, Khalid and Chronopoulou, Alexandra and Currey, Anna and Declerck, Thierry and Dong, Qianqian and Duh, Kevin and Estève, Yannick and Federico, Marcello and Gahbiche, Souhir and Haddow, Barry and Hsu, Benjamin and {Mon Htut}, Phu and Inaguma, Hirofumi and Javorský, Dávid and Judge, John and Kano, Yasumasa and Ko, Tom and Kumar, Rishu and Li, Pengwei and Ma, Xutai and Mathur, Prashant and Matusov, Evgeny and McNamee, Paul and {P. McCrae}, John and Murray, Kenton and Nadejde, Maria and Nakamura, Satoshi and Negri, Matteo and Nguyen, Ha and Niehues, Jan and Niu, Xing and {Kr. Ojha}, Atul and {E. Ortega}, John and Pal, Proyag and Pino, Juan and van der Plas, Lonneke and Polák, Peter and Rippeth, Elijah and Salesky, Elizabeth and Shi, Jiatong and Sperber, Matthias and Stüker, Sebastian and Sudoh, Katsuhito and Tang, Yun and Thompson, Brian and Tran, Kevin and Turchi, Marco and Waibel, Alex and Wang, Mingxuan and Watanabe, Shinji and Zevallos, Rodolfo},
  booktitle       = {Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)},
  eventdate       = {2023-07-13/2023-07-14},
  eventtitle      = {20th International Conference on Spoken Language Translation},
  eventtitleaddon = {IWSLT 2023},
  language        = {english},
  pages           = {1–61},
  publisher       = {{Association for Computational Linguistics (ACL)}},
  title           = {Findings of the IWSLT 2023 Evaluation Campaign},
  venue           = {Toronto, Kanada},
  year            = {2023}
}

@misc{pan-2021-mrasp2,
  archiveprefix = {arXiv},
  author        = {Xiao Pan and Mingxuan Wang and Liwei Wu and Lei Li},
  eprint        = {2105.09501},
  primaryclass  = {cs.CL},
  title         = {Contrastive Learning for Many-to-many Multilingual Neural Machine Translation},
  url           = {https://arxiv.org/abs/2105.09501},
  year          = {2021}
}

@misc{chiang-2022-breaking-multilingual,
  archiveprefix = {arXiv},
  author        = {Ting-Rui Chiang and Yi-Pei Chen and Yi-Ting Yeh and Graham Neubig},
  eprint        = {2110.08130},
  primaryclass  = {cs.CL},
  title         = {Breaking Down Multilingual Machine Translation},
  url           = {https://arxiv.org/abs/2110.08130},
  year          = {2022}
}

@inproceedings{tiedemann-2020-tatoeba-challenge,
  address   = {Online},
  author    = {Tiedemann, J{\"o}rg},
  booktitle = {Proceedings of the Fifth Conference on Machine Translation},
  month     = nov,
  pages     = {1174--1182},
  publisher = {Association for Computational Linguistics},
  title     = {The {T}atoeba {T}ranslation {C}hallenge {--} {R}ealistic Data Sets for Low Resource and Multilingual {MT}},
  url       = {https://www.aclweb.org/anthology/2020.wmt-1.139},
  year      = {2020}
}

@inproceedings{barrault-2020-wmt,
  address   = {Online},
  author    = {Barrault, Lo{\"\i}c  and
               Biesialska, Magdalena  and
               Bojar, Ond{\v{r}}ej  and
               Costa-juss{\`a}, Marta R.  and
               Federmann, Christian  and
               Graham, Yvette  and
               Grundkiewicz, Roman  and
               Haddow, Barry  and
               Huck, Matthias  and
               Joanis, Eric  and
               Kocmi, Tom  and
               Koehn, Philipp  and
               Lo, Chi-kiu  and
               Ljube{\v{s}}i{\'c}, Nikola  and
               Monz, Christof  and
               Morishita, Makoto  and
               Nagata, Masaaki  and
               Nakazawa, Toshiaki  and
               Pal, Santanu  and
               Post, Matt  and
               Zampieri, Marcos},
  booktitle = {Proceedings of the Fifth Conference on Machine Translation},
  editor    = {Barrault, Lo{\"\i}c  and
               Bojar, Ond{\v{r}}ej  and
               Bougares, Fethi  and
               Chatterjee, Rajen  and
               Costa-juss{\`a}, Marta R.  and
               Federmann, Christian  and
               Fishel, Mark  and
               Fraser, Alexander  and
               Graham, Yvette  and
               Guzman, Paco  and
               Haddow, Barry  and
               Huck, Matthias  and
               Yepes, Antonio Jimeno  and
               Koehn, Philipp  and
               Martins, Andr{\'e}  and
               Morishita, Makoto  and
               Monz, Christof  and
               Nagata, Masaaki  and
               Nakazawa, Toshiaki  and
               Negri, Matteo},
  month     = nov,
  pages     = {1--55},
  publisher = {Association for Computational Linguistics},
  title     = {Findings of the 2020 Conference on Machine Translation ({WMT}20)},
  url       = {https://aclanthology.org/2020.wmt-1.1},
  year      = {2020}
}

@misc{rei-2020-comet,
  archiveprefix = {arXiv},
  author        = {Ricardo Rei and Craig Stewart and Ana C Farinha and Alon Lavie},
  eprint        = {2009.09025},
  primaryclass  = {cs.CL},
  title         = {COMET: A Neural Framework for MT Evaluation},
  url           = {https://arxiv.org/abs/2009.09025},
  year          = {2020}
}

@misc{post-2018-sacrebleu,
  archiveprefix = {arXiv},
  author        = {Matt Post},
  eprint        = {1804.08771},
  primaryclass  = {cs.CL},
  title         = {A Call for Clarity in Reporting BLEU Scores},
  url           = {https://arxiv.org/abs/1804.08771},
  year          = {2018}
}

@inproceedings{callison-burch-2006-reevaluating-bleu,
  address   = {Trento, Italy},
  author    = {Callison-Burch, Chris  and
               Osborne, Miles  and
               Koehn, Philipp},
  booktitle = {11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics},
  editor    = {McCarthy, Diana  and
               Wintner, Shuly},
  month     = apr,
  pages     = {249--256},
  publisher = {Association for Computational Linguistics},
  title     = {Re-evaluating the Role of {B}leu in Machine Translation Research},
  url       = {https://aclanthology.org/E06-1032},
  year      = {2006}
}

@inbook{lefer-2020-parallel-corpora,
  address   = {Cham},
  author    = {Lefer, Marie-Aude},
  booktitle = {A Practical Handbook of Corpus Linguistics},
  doi       = {10.1007/978-3-030-46216-1_12},
  editor    = {Paquot, Magali
               and Gries, Stefan Th.},
  isbn      = {978-3-030-46216-1},
  pages     = {257--282},
  publisher = {Springer International Publishing},
  title     = {Parallel Corpora},
  url       = {https://doi.org/10.1007/978-3-030-46216-1_12},
  year      = {2020}
}

@article{rosa-2017-indirect-translation-problems,
  author    = {Alexandra Assis Rosa, Hanna Pięta and Rita Bueno Maia},
  doi       = {10.1080/14781700.2017.1285247},
  eprint    = {https://doi.org/10.1080/14781700.2017.1285247},
  journal   = {Translation Studies},
  number    = {2},
  pages     = {113--132},
  publisher = {Routledge},
  title     = {Theoretical, methodological and terminological issues regarding indirect translation: An overview},
  url       = {https://doi.org/10.1080/14781700.2017.1285247},
  volume    = {10},
  year      = {2017}
}

@inproceedings{agarwal-2008-meteor-mbleu-mter,
  address   = {USA},
  author    = {Agarwal, Abhaya and Lavie, Alon},
  booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
  isbn      = {9781932432091},
  location  = {Columbus, Ohio},
  numpages  = {4},
  pages     = {115–118},
  publisher = {Association for Computational Linguistics},
  series    = {StatMT '08},
  title     = {METEOR, M-BLEU and M-TER: evaluation metrics for high-correlation with human rankings of machine translation output},
  year      = {2008}
}

@misc{huggingface,
  author       = {Hugging Face},
  howpublished = {\url{https://huggingface.co}},
  note         = {Accessed: 2024-08-07},
  title        = {Hugging Face: Natural Language Processing Made Easy},
  year         = {2024}
}

@inproceedings{wolf-etal-2020-transformers,
  address   = {Online},
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  pages     = {38--45},
  publisher = {Association for Computational Linguistics},
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  year      = {2020}
}

@inproceedings{ott-2019-fairseq,
  author    = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  title     = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  year      = {2019}
}

@inproceedings{mariannmt,
  address   = {Melbourne, Australia},
  author    = {Junczys-Dowmunt, Marcin and Grundkiewicz, Roman and
               Dwojak, Tomasz and Hoang, Hieu and Heafield, Kenneth and
               Neckermann, Tom and Seide, Frank and Germann, Ulrich and
               Fikri Aji, Alham and Bogoychev, Nikolay and
               Martins, Andr\'{e} F. T. and Birch, Alexandra},
  booktitle = {Proceedings of ACL 2018, System Demonstrations},
  month     = {July},
  pages     = {116--121},
  publisher = {Association for Computational Linguistics},
  title     = {Marian: Fast Neural Machine Translation in {C++}},
  url       = {http://www.aclweb.org/anthology/P18-4020},
  year      = {2018}
}

@misc{arivazhagan-2019-massively-multilingual,
  archiveprefix = {arXiv},
  author        = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Dmitry Lepikhin and Melvin Johnson and Maxim Krikun and Mia Xu Chen and Yuan Cao and George Foster and Colin Cherry and Wolfgang Macherey and Zhifeng Chen and Yonghui Wu},
  eprint        = {1907.05019},
  primaryclass  = {cs.CL},
  title         = {Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
  url           = {https://arxiv.org/abs/1907.05019},
  year          = {2019}
}

@article{masoudnia-2012-moe,
  author    = {Masoudnia,  Saeed and Ebrahimpour,  Reza},
  doi       = {10.1007/s10462-012-9338-y},
  issn      = {1573-7462},
  journal   = {Artificial Intelligence Review},
  month     = may,
  number    = {2},
  pages     = {275–293},
  publisher = {Springer Science and Business Media LLC},
  title     = {Mixture of experts: a literature survey},
  url       = {http://dx.doi.org/10.1007/s10462-012-9338-y},
  volume    = {42},
  year      = {2012}
}

@misc{zhang-2020-bertscore,
  archiveprefix = {arXiv},
  author        = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  eprint        = {1904.09675},
  primaryclass  = {cs.CL},
  title         = {BERTScore: Evaluating Text Generation with BERT},
  url           = {https://arxiv.org/abs/1904.09675},
  year          = {2020}
}

@book{bird-2009-natural,
  author    = {Bird, Steven and Loper, Edward and Klein, Ewan},
  publisher = {O'Reilly Media Inc.},
  title     = {Natural Language Processing with Python},
  year      = {2009}
}

@inproceedings{zhang-2023-fine-tuning,
  abstract  = {While large language models have made remarkable advancements in natural language generation, their potential in machine translation, especially when fine-tuned, remains under-explored. In our study, we conduct comprehensive experiments, evaluating 15 publicly available language models on machine translation tasks. We compare the performance across three methodologies: zero-shot prompting, few-shot learning, and fine-tuning. Central to our approach is the use of QLoRA, an efficient fine-tuning method. On French-English, QLoRA fine-tuning outperforms both few-shot learning and models trained from scratch. This superiority is highlighted in both sentence-level and document-level translations, with a significant BLEU score improvement of 28.93 over the prompting method. Impressively, with QLoRA, the enhanced performance is achieved by fine-tuning a mere 0.77{\%} of the model{'}s parameters.},
  address   = {Singapore},
  author    = {Zhang, Xuan  and
               Rajabi, Navid  and
               Duh, Kevin  and
               Koehn, Philipp},
  booktitle = {Proceedings of the Eighth Conference on Machine Translation},
  doi       = {10.18653/v1/2023.wmt-1.43},
  editor    = {Koehn, Philipp  and
               Haddow, Barry  and
               Kocmi, Tom  and
               Monz, Christof},
  month     = dec,
  pages     = {468--481},
  publisher = {Association for Computational Linguistics},
  title     = {Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with {QL}o{RA}},
  url       = {https://aclanthology.org/2023.wmt-1.43},
  year      = {2023}
}

@misc{bahdanau-2016-nmt-jointly,
  archiveprefix = {arXiv},
  author        = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  eprint        = {1409.0473},
  primaryclass  = {cs.CL},
  title         = {Neural Machine Translation by Jointly Learning to Align and Translate},
  url           = {https://arxiv.org/abs/1409.0473},
  year          = {2016}
}

@misc{luong-2015-effective-attention,
  archiveprefix = {arXiv},
  author        = {Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
  eprint        = {1508.04025},
  primaryclass  = {cs.CL},
  title         = {Effective Approaches to Attention-based Neural Machine Translation},
  url           = {https://arxiv.org/abs/1508.04025},
  year          = {2015}
}

@article{han-2021-ptms,
  abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
  author   = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  doi      = {https://doi.org/10.1016/j.aiopen.2021.08.002},
  issn     = {2666-6510},
  journal  = {AI Open},
  keywords = {Pre-trained models, Language models, Transfer learning, Self-supervised learning, Natural language processing, Multimodal processing, Artificial intelligence},
  pages    = {225-250},
  title    = {Pre-trained models: Past, present and future},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
  volume   = {2},
  year     = {2021}
}